{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy 1.26.4\n",
      "Pandas 2.2.2\n",
      "Matplotlib 3.9.2\n",
      "Python 3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib # type: ignore\n",
    "import sys\n",
    "import nnfs # type: ignore\n",
    "import math as m\n",
    "print(\"Numpy\", np.__version__)\n",
    "print(\"Pandas\", pd.__version__)\n",
    "print(\"Matplotlib\", matplotlib.__version__)\n",
    "print(\"Python\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neuron üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.7\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.2, 5.1, 2.1] # 3 inputs are the three neurons from previous layer\n",
    "weights= [3.1, 2.1, 8.7] # 3 weights are the weights of the three neurons from previous layer\n",
    "bias = 3 # bias is the bias of the neuron from the previous layer\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias  # output is the output of the neuron from the previous layer\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate a Layer ü•û"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four Inputs into Four Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5] # 3 inputs are the four neurons from previous layer\n",
    "weights= [0.2, 0.8, -0.5, 1.0] # 3 weights are the weights of the four neurons from previous layer\n",
    "bias = 2 # bias is the bias of the neuron from the previous layer\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3]+ bias  # output is the output of the neuron from the previous layer\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four Inputs into Three Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5] # 4 inputs are the four neurons from previous layer\n",
    "\n",
    "weights1= [0.2, 0.8, -0.5, 1.0] # 3 weights are the weights of the four neurons from previous layer\n",
    "weights2= [0.5, -0.91, 0.26, -0.5]\n",
    "weights3= [-0.26, -0.27, 0.17, 0.87]\n",
    "bias1 = 2 # bias is the bias of the neuron from the previous layer\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3]+ bias1, \n",
    "          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3]+ bias2,\n",
    "          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3]+ bias3]  # output is the layered output of the neurons from the previous layer\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5] # 4 inputs are the four neurons from previous layer\n",
    "weights= [[0.2, 0.8, -0.5, 1.0],\n",
    "          [0.5, -0.91, 0.26, -0.5],\n",
    "          [-0.26, -0.27, 0.17, 0.87]] # 3 weights are the weights of the four neurons from previous layer\n",
    "biases = [2, 3, 0.5] # bias is the bias of the neuron from the previous layer\n",
    "\n",
    "layer_outputs = [] # Output of current layer\n",
    "for neuron_weights, neuron_bias in zip(weights, biases): #Zip is used to iterate over two lists at the same time and make a list of lists\n",
    "    neuron_output = 0 # Output of given neuron\n",
    "    for n_input, weight in zip(inputs, neuron_weights): #Zip is used to iterate over two lists at the same time\n",
    "        neuron_output += n_input*weight # Multiply the input with the weight and add it to the neuron_output\n",
    "    neuron_output += neuron_bias # Add the bias to the neuron_output\n",
    "    layer_outputs.append(neuron_output) # Append the neuron_output to the layer_outputs\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot Product and Matrix Product üîÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights= [0.2, 0.8, -0.5, 1.0] # 3 weights are the weights of the four neurons from previous layer\n",
    "bias = 2 # bias is the bias of the neuron from the previous layer\n",
    "\n",
    "output = np.dot(weights, inputs) + bias # np.dot is used to multiply the weights and inputs and add them together\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5] # 4 inputs are the four neurons from previous layer\n",
    "weights= [[0.2, 0.8, -0.5, 1.0],\n",
    "          [0.5, -0.91, 0.26, -0.5],\n",
    "          [-0.26, -0.27, 0.17, 0.87]] # 3 weights are the weights of the four neurons from previous layer\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "output = np.dot(weights, inputs) + biases # np.dot is used to multiply the weights and inputs and add them together\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Sizing üì¶üî¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]] # inputs are the four neurons from previous layer(3,4)\n",
    "\n",
    "weights= [[0.2, 0.8, -0.5, 1.0],\n",
    "          [0.5, -0.91, 0.26, -0.5],\n",
    "          [-0.26, -0.27, 0.17, 0.87]] # weights are the weights of the four neurons from previous layer(3,4)\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "# output = np.dot(weights, inputs) + biases # np.dot is used to multiply the weights and inputs and add them together(3,4) * (3,4) = Error\n",
    "output = np.dot(inputs, np.array(weights).T) + biases # np.dot is used to multiply the weights and inputs and add them together(3,4) * (4,3) = (3,3)\n",
    "# T or Transpose is used to convert the weights from (3,4) to (4,3) to make the multiplication possible\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiLayered Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]] # inputs are the four neurons from previous layer(3,4)\n",
    "\n",
    "weights= [[0.2, 0.8, -0.5, 1.0],\n",
    "          [0.5, -0.91, 0.26, -0.5],\n",
    "          [-0.26, -0.27, 0.17, 0.87]] # weights are the weights of the four neurons from previous layer(3,4)\n",
    "\n",
    "weights2= [[0.1, -0.14, 0.5],\n",
    "          [-0.5, 0.12, -0.33],\n",
    "          [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "# output = np.dot(weights, inputs) + biases # np.dot is used to multiply the weights and inputs and add them together(3,4) * (3,4) = Error\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases # np.dot is used to multiply the weights and inputs and add them together(3,4) * (4,3) = (3,3)\n",
    "# T or Transpose is used to convert the weights from (3,4) to (4,3) to make the multiplication possible\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectification of the Layers üõ§Ô∏èüìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 Output: [[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "X = [[1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]] # X denotes inputs\n",
    "\n",
    "np.random.seed(0) # Seed is used to generate the same random numbers every time\n",
    "\n",
    "\n",
    "class Layer_Dense: # Class for the dense layer- Hidden layer\n",
    "    def __init__(self, n_inputs, n_neurons): #Entry point for the class object\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) # Random weights of the neurons\n",
    "        self.biases = np.zeros((1, n_neurons)) # Zero biases of the neurons\n",
    "    def forward(self, inputs): # Forward function to calculate the output\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases # Output of the neurons\n",
    "\n",
    "layer1 = Layer_Dense(4,5) # Layer 1 with 4 inputs and 5 neurons\n",
    "layer2 = Layer_Dense(5,2) # Layer 2 with 5 inputs and 2 neurons\n",
    "layer1.forward(X) # Forward function of layer 1\n",
    "layer2.forward(layer1.output) # Forward function of layer 2\n",
    "\n",
    "\n",
    "print(\"Layer 2 Output:\",layer2.output) # Output of layer 1\n",
    "# print(0.10*np.random.randn(4, 3)) # Random weights of the neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions ‚ö°üìàüß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU- Rectified Linear Unit  Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "X = [[1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100] \n",
    "\n",
    "output= []\n",
    "\n",
    "# for i in inputs:\n",
    "#     if i > 0:\n",
    "#         output.append(i)\n",
    "#     elif i <= 0:\n",
    "#         output.append(0)\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()#setting the random seed through nnfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from nnfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data # type: ignore\n",
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.        ]\n",
      " [0.         0.00027121 0.         0.00026233 0.        ]\n",
      " [0.         0.         0.         0.00029877 0.        ]\n",
      " ...\n",
      " [0.15903221 0.         0.1166153  0.         0.14184746]\n",
      " [0.16403008 0.         0.15140174 0.         0.14257608]\n",
      " [0.15084316 0.         0.0833516  0.         0.13780963]]\n"
     ]
    }
   ],
   "source": [
    "class Layer_Dense: # Class for the dense layer- Hidden layer\n",
    "    def __init__(self, n_inputs, n_neurons): #Entry point for the class object\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) # Random weights of the neurons\n",
    "        self.biases = np.zeros((1, n_neurons)) # Zero biases of the neurons\n",
    "    def forward(self, inputs): # Forward function to calculate the output\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases # Output of the neurons\n",
    "\n",
    "class Activation_ReLU: # Class for the ReLU activation function\n",
    "    def forward(self, inputs): # Forward function to calculate the output\n",
    "        self.output = np.maximum(0, inputs) # Output of the ReLU activation function\n",
    "\n",
    "layer1 = Layer_Dense(2,5) # Layer 1 with 2 inputs and 5 neurons\n",
    "activation1 = Activation_ReLU() # ReLU activation function object\n",
    "layer1.forward(X) # Forward function of layer 1\n",
    "activation1.forward(layer1.output) # Forward function of ReLU activation function\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "E = m.e # Euler's number \n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output) # Exponential of the output\n",
    "\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "norm_base = sum(exp_values) # Sum of the exponential values\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value/norm_base) # Normalized values\n",
    "print(norm_values)\n",
    "print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89528266 0.02470831 0.08000903]\n"
     ]
    }
   ],
   "source": [
    "exp_values = np.exp(layer_outputs) # Exponential of the output\n",
    "norm_values = exp_values / np.sum(exp_values) # Normalized values\n",
    "print(norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftMax= Input -> Exponentiate -> Normalize -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [[4.8, 1.21, 2.385],\n",
    "                [8.9, -1.81, 0.2],\n",
    "                [1.41, 1.051, 0.026]]\n",
    "exp_values = np.exp(layer_outputs)\n",
    "norm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True) # Normalized values\n",
    "print(norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33335057 0.33332202 0.33332744]\n",
      " [0.3333651  0.3333086  0.33332625]\n",
      " [0.33336136 0.3332889  0.3333498 ]\n",
      " [0.33331835 0.33321413 0.3334675 ]]\n"
     ]
    }
   ],
   "source": [
    "class Layer_Dense: # Class for the dense layer- Hidden layer\n",
    "    def __init__(self, n_inputs, n_neurons): #Entry point for the class object\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons) # Random weights of the neurons\n",
    "        self.biases = np.zeros((1, n_neurons)) # Zero biases of the neurons\n",
    "    def forward(self, inputs): # Forward function to calculate the output\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases # Output of the neurons\n",
    "\n",
    "class Activation_ReLU: # Class for the ReLU activation function\n",
    "    def forward(self, inputs): # Forward function to calculate the output\n",
    "        self.output = np.maximum(0, inputs) # Output of the ReLU activation function\n",
    "\n",
    "class Activation_Softmax: # Class for the Softmax activation function\n",
    "    def forward(self, inputs): # Forward function to calculate the output\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # Exponential of the output\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True) # Normalized values\n",
    "        self.output = probabilities\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3) # Spiral data\n",
    "\n",
    "dense1 = Layer_Dense(2,3) # Layer 1 with 2 inputs and 3 neurons\n",
    "activation1 = Activation_ReLU() # ReLU activation function object\n",
    "\n",
    "dense2 = Layer_Dense(3,3) # Layer 2 with 3 inputs and 3 neurons\n",
    "activation2 = Activation_Softmax() # Softmax activation function object\n",
    "\n",
    "dense1.forward(X) # Forward function of layer 1\n",
    "activation1.forward(dense1.output) # Forward function of ReLU activation function  \n",
    "\n",
    "dense2.forward(activation1.output) # Forward function of layer 2\n",
    "activation2.forward(dense2.output) # Forward function of Softmax activation function\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Loss with Categorical Cross-Entropy üßÆüéØüìâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n"
     ]
    }
   ],
   "source": [
    "b= 5.2\n",
    "print(np.log(b)) # Natural logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.199999999999999"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.e ** 1.6486586255873816 # Euler's number raised to the power of the natural logarithm of 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "softmax_output = [0.7, 0.1, 0.2] # Softmax output\n",
    "target_output = [1, 0, 0] # Target output\n",
    "\n",
    "Loss = -(m.log(softmax_output[0])*target_output[0] + \n",
    "         m.log(softmax_output[1])*target_output[1] + \n",
    "         m.log(softmax_output[2])*target_output[2]) # Loss function\n",
    "\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "loss = -m.log(softmax_output[0]) # Loss function\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
